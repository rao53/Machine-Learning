{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question 1, Number 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d-fold cross-validation means we use d subsets of data to get the value of k.\n",
    "If the dataset is split into training data and testing data in the ratio of 80:20, we have 80% of data in training data(D_Train). We further randomly split the training data into 4 equal parts(D1, D2, D3, D4). \n",
    "\n",
    "By keeping one of the datasets as a cross-validation dataset and train the model using the remaining datasets for different values of k and find the best value of k.\n",
    "Here's how to select k value using d-fold cross-validation. It's usually called k-fold, but we're using k to talk about the number of neighbors.\n",
    "\n",
    "1. Split the data into d disjoint, similarly-sized subsets\n",
    "2. Hold out the first set. This is called the validation set.\n",
    "3. Train your classifier on the remaining data. In the case of knn classification, just remember all the data.\n",
    "4. For each value of k:\n",
    "    - Classify each point in the validation set, using its k nearest neighbors in the training set\n",
    "    - Record the accuracy\n",
    "5. Repeat steps 1-4 for all d choices of the validation set.\n",
    "6. For each choice of k, find the average accuracy across validation sets. Choose the value of k with the highest accuracy.\n",
    "7. Construct a final classifier using all of the original data and the chosen value of k. This is what you'd use to classify new points.\n",
    "\n",
    "Only one drawback of using cross-validation is we are repeating computations for every k. In spite of having high time complexity, the process is worth it as it increases the generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My final model: y = 2.1434 + 3.7737 x1 + -2.5566 x2\n",
      "Co-efficients from skLearn theta1 = 69.7372, theta2 = -30.9021 \n",
      "Intercept from skLearn = 49.2632 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def logisticRegression(learn_rate, num_iter):\n",
    "    # Read in data, ignore first row (header info) and first column (index info)\n",
    "    iris = load_iris()\n",
    "    X = iris.data[ : , :2]\n",
    "    y = (iris.target!= 0) * 1\n",
    "    \n",
    "    num_rows, num_cols = X.shape\n",
    "    \n",
    "    # Get the mean and standard deviation\n",
    "    mean = X.mean(axis=0)\n",
    "    std = np.std(X, axis=0, ddof=1)\n",
    "    \n",
    "    #Standadized train and test X vector with bias\n",
    "    sXNoBias = (X - mean)/std    \n",
    "    sX = np.column_stack((np.ones(num_rows), sXNoBias))\n",
    "\n",
    "    #Initialize the parameters of theta using random values in the range [-1,1]\n",
    "    theta = np.random.uniform(low=-1, high=1,size=(num_cols+1)).reshape(((num_cols+1)),1)\n",
    "#     theta = [ [0.02864473], [-0.70407966], [-0.05586018]]\n",
    "\n",
    "    finalNumIter = 0\n",
    "    y = y.reshape(num_rows, 1)\n",
    "\n",
    "#     terminate when abs val in percent chage in loss on the data is < 2^-23 or num_iter iterations\n",
    "    allLosses = []\n",
    "    for i in range(num_iter):\n",
    "        finalNumIter += 1\n",
    "        init = (sX @ theta)\n",
    "        expInit = np.exp(-init)\n",
    "        sigmoid = 1/(1 + expInit)\n",
    "        inBracket = sigmoid - y\n",
    "        rhs = (sX.T @ inBracket)\n",
    "        theta = theta - ((learn_rate * rhs)/num_rows)\n",
    "        yHat = sX @ theta\n",
    "        loss = (yHat * np.log(sigmoid)) + ((1-yHat) * np.log(1-sigmoid))\n",
    "        loss = loss.sum(axis=0)\n",
    "        allLosses.append(loss)\n",
    "        \n",
    "        #if need to stop for RMSE train percent change less than 2^-23\n",
    "        if i < 2:\n",
    "            #For empty loss function array\n",
    "            continue\n",
    "        else:\n",
    "            lossLen = len(allLosses)\n",
    "            diff = allLosses[lossLen-1] - allLosses[lossLen-2] #new - old\n",
    "            incr = abs((diff/allLosses[lossLen-2]) * 100)\n",
    "            compr = 2 ** -23\n",
    "            if incr < compr:\n",
    "                break\n",
    "  \n",
    "    #Final thetas\n",
    "    print(\"My final model: y = %.4f + %.4f x1 + %.4f x2\" %(theta[0], theta[1], theta[2]))\n",
    "    lgr = LogisticRegression(penalty='none', solver='lbfgs', max_iter=10000)\n",
    "    y = (iris.target!= 0) * 1\n",
    "    lgr.fit(sXNoBias,y)\n",
    "    print(\"Co-efficients from skLearn theta1 = %.4f, theta2 = %.4f \" %(lgr.coef_[0][0], lgr.coef_[0][1]))\n",
    "    print(\"Intercept from skLearn = %.4f \" %(lgr.intercept_[0]))\n",
    "    \n",
    "#     x_something is feature 1\n",
    "    feature2 = X[:,1]\n",
    "    x_mine = -(theta[0]/theta[1])-((theta[2]/theta[1])*4)\n",
    "    x_mine_2 = -(theta[0]/theta[1])-((theta[2]/theta[1])*8)\n",
    "    \n",
    "    \n",
    "    x_logistic = -(lgr.intercept_[0]/lgr.coef_[0][0])-((lgr.coef_[0][1]/lgr.coef_[0][0])*4)\n",
    "    x_logistic_2 = -(lgr.intercept_[0]/lgr.coef_[0][0])-((lgr.coef_[0][1]/lgr.coef_[0][0])*15)\n",
    "    \n",
    "    #x-axis = feature 1 and y-axis = feature 2\n",
    "    plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', marker='*')\n",
    "    plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='g', marker='o')\n",
    "        \n",
    "    plt.plot([4, 8],[x_mine, x_mine_2],label=\"my_method\")   \n",
    "    plt.plot([4, 8],[x_logistic, x_logistic_2], label=\"skLearn_method\")   \n",
    "    plt.xlabel('feature1')\n",
    "    plt.ylabel('feature2')\n",
    "\n",
    "    plt.title(\"Plot of feature1 vs feature2\")\n",
    "    plt.legend()\n",
    "\n",
    "logisticRegression(0.01, 10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.8893 \n",
      "Recall = 0.8218 \n",
      "F_measure = 0.8542 \n",
      "Accuracy = 0.8890 \n"
     ]
    }
   ],
   "source": [
    "def LogisticRegressionSpam(filename,learn_rate, num_iter):\n",
    "    # Read in data, ignore first row (header info) and first column (index info)\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    num_rows, num_cols = df.shape\n",
    "    \n",
    "    # Randomize the data (Gives the same data everytime)\n",
    "    df = df.sample(n = num_rows, random_state=0)\n",
    "    \n",
    "    # Select 2/3 of the data for training and the rest for testing\n",
    "    train = df.sample(frac=0.667, random_state=0)\n",
    "    test = df.drop(train.index)\n",
    "    \n",
    "    # Standadize the data... Make sure you retain the mean and standard deviation\n",
    "    mean = np.mean(df).iloc[:-1]\n",
    "    std = np.std(df, axis=0, ddof=1).iloc[:-1]\n",
    "    trainXVec = train.iloc[: , :-1] #X-vector train\n",
    "    testXVec = test.iloc[: , :-1] #X-vector test\n",
    "    trainYVec = train.iloc[: , -1:] #Y-vector train\n",
    "    testYVec = test.iloc[: , -1:] #Y-vector test\n",
    "    sTrainXVec = (trainXVec - mean)/std    \n",
    "    sTestXVec = (testXVec - mean)/std\n",
    "    sTrainNoBias = sTrainXVec\n",
    "    sTrainXVec.insert(0,'Bias',1)\n",
    "    sTestXVec.insert(0,'Bias',1)\n",
    "    num_rows_train, num_cols_train = sTrainXVec.shape\n",
    "    num_rows_test, num_cols_train = sTestXVec.shape\n",
    "    \n",
    "    \n",
    "    #Initialize the parameters of theta using random values in the range [-1,1]\n",
    "    theta = np.random.uniform(low=-1, high=1,size=(num_cols)).reshape(((num_cols)),1)\n",
    "    finalNumIter = 0\n",
    "    \n",
    "    small_random_number = np.finfo(float).eps\n",
    "\n",
    "#     terminate when abs val in percent chage in loss on the data is < 2^-23 or num_iter iterations\n",
    "    allLosses = []\n",
    "    for i in range(num_iter):\n",
    "        finalNumIter += 1\n",
    "        init = (sTrainXVec @ theta)\n",
    "        expInit = np.exp(-init)\n",
    "        sigmoid = (1/(1 + expInit))\n",
    "        inBracket = np.array(sigmoid) - np.array(trainYVec)\n",
    "        rhs = (sTrainXVec.T @ inBracket)\n",
    "        theta = theta - ((learn_rate * rhs)/num_rows_train)\n",
    "        yHat = sTrainXVec @ theta\n",
    "        loss = (yHat * np.log(sigmoid + small_random_number)) + ((1-yHat) * np.log(1-sigmoid + small_random_number))\n",
    "        loss = loss.sum(axis=0)\n",
    "        allLosses.append(loss)\n",
    "        \n",
    "        #if need to stop for loss percent change less than 2^-23\n",
    "        if i < 2:\n",
    "            #For empty loss function array\n",
    "            continue\n",
    "        else:\n",
    "            lossLen = len(allLosses)\n",
    "            diff = allLosses[lossLen-1] - allLosses[lossLen-2] #new - old\n",
    "            incr = abs((diff/allLosses[lossLen-2]) * 100)\n",
    "            compr = 2 ** -23\n",
    "            if incr[0] < compr:\n",
    "                break\n",
    "\n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    init = (sTestXVec @ theta)\n",
    "    expInit = np.exp(-init)\n",
    "    sigmoid_test = np.array((1/(1 + expInit)))\n",
    "    prob_yEq1 = sigmoid_test\n",
    "    prob_yEq0 = (1 - sigmoid_test)\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(num_rows_test):\n",
    "        spam = prob_yEq1[i] ##\n",
    "        not_spam = prob_yEq0[i]  ##\n",
    "        if spam > not_spam:\n",
    "            y_pred.append(1)\n",
    "            if testYVec.iloc[i,0] == 1:\n",
    "                TP = TP + 1\n",
    "            else:\n",
    "                FP = FP + 1\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            if testYVec.iloc[i,0] == 0:\n",
    "                TN = TN + 1\n",
    "            else:\n",
    "                FN = FN + 1 \n",
    "            \n",
    "            \n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    F_measure = (2 * Precision * Recall)/(Precision + Recall)\n",
    "    Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    \n",
    "    print(\"Precision = %.4f \" %(Precision))\n",
    "    print(\"Recall = %.4f \" %(Recall))\n",
    "    print(\"F_measure = %.4f \" %(F_measure))\n",
    "    print(\"Accuracy = %.4f \" %(Accuracy))\n",
    "    \n",
    "\n",
    "# ----------------------------\n",
    "    \n",
    "LogisticRegressionSpam(\"spambase.data\", 0.01, 1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7005 \n",
      "Recall = 0.9571 \n",
      "F_measure = 0.8089 \n",
      "Accuracy = 0.8211 \n",
      "--------------------\n",
      "Number of mislabeled points out of a total 1532 points : 274\n"
     ]
    }
   ],
   "source": [
    "def naiveBayes(filename):\n",
    "    # Read in data, ignore first row (header info) and first column (index info)\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    num_rows, num_cols = df.shape\n",
    "    \n",
    "    # Randomize the data (Gives the same data everytime)\n",
    "    df = df.sample(n = num_rows, random_state=0)\n",
    "    \n",
    "    # Select 2/3 of the data for training and the rest for testing\n",
    "    train = df.sample(frac=0.667, random_state=0)\n",
    "    test = df.drop(train.index)\n",
    "    \n",
    "    # Standadize the data... Make sure you retain the mean and standard deviation\n",
    "    mean = np.mean(df).iloc[:-1]\n",
    "    std = np.std(df, axis=0, ddof=1).iloc[:-1]\n",
    "    trainXVec = train.iloc[: , :-1] #X-vector train\n",
    "    testXVec = test.iloc[: , :-1] #X-vector test\n",
    "    trainYVec = train.iloc[: , -1:] #Y-vector train\n",
    "    testYVec = test.iloc[: , -1:] #Y-vector test\n",
    "    sTrainXVec = (trainXVec - mean)/std    \n",
    "    sTestXVec = (testXVec - mean)/std\n",
    "    \n",
    "    #combine standardized X-vector of train with untouched Y-vector (contains both spam and not_spam)\n",
    "    train = sTrainXVec.join(trainYVec)\n",
    "    num_rows_train, num_cols_train = train.shape\n",
    "    num_rows_test, num_cols_test = sTestXVec.shape\n",
    "    \n",
    "    #Divide training data into spam and non spam\n",
    "    spam = train[train.iloc[:,-1] == 1] #with values 1\n",
    "    notSpam = train[train.iloc[:,-1] == 0] #with values 0\n",
    "    \n",
    "    \n",
    "    #Create normal models for each feature\n",
    "    spamMean = np.mean(spam).iloc[:-1]\n",
    "    spamStd = np.std(spam, axis=0, ddof=1).iloc[:-1]\n",
    "    notSpamMean = np.mean(notSpam).iloc[:-1]\n",
    "    notSpamStd = np.std(notSpam, axis=0, ddof=1).iloc[:-1]\n",
    "    \n",
    "    #classify each testing sample using the normal model above and choose which class probability is higher\n",
    "    num_rows_spam, num_cols_spam = spam.shape\n",
    "    num_rows_Notspam, num_cols_Notspam = notSpam.shape\n",
    "    \n",
    "    #Get prior probability\n",
    "    prob_spam = num_rows_spam/num_rows_train    \n",
    "    prob_Not_spam = num_rows_Notspam/num_rows_train\n",
    "    \n",
    "    #Get spam posterior probability\n",
    "    #1.) get the likelihood using gaussian PDF - For Spam (Xk is Standadized testData X-vector only)\n",
    "    up = np.exp(-((sTestXVec-spamMean) ** 2)/(2*(spamStd ** 2)))\n",
    "    down = ((math.sqrt(2*math.pi)) * spamStd)\n",
    "    total = up/down\n",
    "    posterior_spam = total.product(axis=1) * prob_spam\n",
    "\n",
    "    #Get Not-spam posterior probability\n",
    "    #2.) get the likelihood using gaussian PDF - For Not Spam (Xk is Standadized testData X-vector only)\n",
    "    up = np.exp(-((sTestXVec-notSpamMean) ** 2)/(2*(notSpamStd ** 2)))\n",
    "    down = ((math.sqrt(2*math.pi)) * notSpamStd)\n",
    "    total = up/down\n",
    "    posterior_Notspam = total.product(axis=1) * prob_Not_spam\n",
    "    \n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    \n",
    "    ##Get the testing statistics\n",
    "    y_pred = []\n",
    "    for i in range(num_rows_test):\n",
    "        spam = posterior_spam.iloc[i]\n",
    "        not_spam = posterior_Notspam.iloc[i]\n",
    "        if spam > not_spam:\n",
    "            y_pred.append(1)\n",
    "            if testYVec.iloc[i,0] == 1:\n",
    "                TP = TP + 1\n",
    "            else:\n",
    "                FP = FP + 1\n",
    "        else:\n",
    "            y_pred.append(0)\n",
    "            if testYVec.iloc[i,0] == 0:\n",
    "                TN = TN + 1\n",
    "            else:\n",
    "                FN = FN + 1 \n",
    "            \n",
    "            \n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    F_measure = (2 * Precision * Recall)/(Precision + Recall)\n",
    "    Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    print(\"Precision = %.4f \" %(Precision))\n",
    "    print(\"Recall = %.4f \" %(Recall))\n",
    "    print(\"F_measure = %.4f \" %(F_measure))\n",
    "    print(\"Accuracy = %.4f \" %(Accuracy))\n",
    "    \n",
    "# ---------------------------------------------------------------------------------------------------------\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (num_rows_test, (np.ravel(testYVec) != y_pred).sum())) \n",
    "\n",
    "    \n",
    "naiveBayes(\"spambase.data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.7966 \n",
      "Recall = 0.8465 \n",
      "F_measure = 0.8208 \n",
      "Accuracy = 0.8538 \n",
      "--------------------\n",
      "Number of mislabeled points out of a total 1532 points : 224\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# To help with building the tree\n",
    "\n",
    "class Node:\n",
    "\n",
    "    def __init__(self, value = None): #This is the constructor\n",
    "        self.value = value\n",
    "        self.left = None #either a leaf node for zero or another Node itself\n",
    "        self.right = None #either a leaf node for zero or another Node itself\n",
    "\n",
    "    def __str__(self): #This prints out a given node.\n",
    "        return \"[\"+str(self.value)+\"]\"\n",
    "    \n",
    "    def getValue(self): #This is a mutator that gets a given node\n",
    "        return self.value\n",
    "    \n",
    "    def setValue(self, value): #This is a mutator that gets a given node\n",
    "        self.value = value\n",
    "\n",
    "    def getLeft(self): #This is a mutator that gets a given node\n",
    "        return self.left\n",
    "    \n",
    "    def setLeft(self, lt): #This is a mutator that gets a given node\n",
    "        self.left = lt\n",
    "        \n",
    "    def getRight(self): #This is a mutator that gets a given node\n",
    "        return self.right\n",
    "    \n",
    "    def setRight(self, rt): #This is a mutator that gets a given node\n",
    "        self.right = rt\n",
    "    \n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    \n",
    "    def __init__(self): #This is the constructor\n",
    "        self.tree = None\n",
    "        self.trainingData = None\n",
    "        \n",
    "    #returns tree that would help me with prediction\n",
    "    def train(self, examples, attributes, default):\n",
    "        xVector = examples.iloc[: , :-1] #X-vector train\n",
    "        yVector = examples.iloc[: , -1:] #Y-vector train\n",
    "        a = (yVector).to_numpy() \n",
    "        \n",
    "#         #base condition \n",
    "        classification = Node()\n",
    "        if ((len(examples) == 0) or (examples.empty)):\n",
    "            classification.setValue(default)\n",
    "            return classification\n",
    "        elif ((a[0] == a).all()):\n",
    "            classification.setValue(a[0])\n",
    "            return classification\n",
    "        elif (len(attributes) == 1):\n",
    "            classification.setValue(self.mode(yVector))\n",
    "            return classification\n",
    "        else:\n",
    "            best_feature, indexOfBest = self.choose_bestAttribute(attributes, examples)\n",
    "            unique_values_best = pd.unique(examples[best_feature])\n",
    "            attributes.pop(indexOfBest)\n",
    "            try: \n",
    "                examples_0 = examples.loc[examples[best_feature] == unique_values_best[0]]\n",
    "                examples_0 = examples_0.drop(best_feature, 1)\n",
    "                yVector0 = examples_0.iloc[: , -1:]\n",
    "                modeExamples0 = self.mode(yVector0)\n",
    "                classification.setLeft(self.train(examples_0, attributes,modeExamples0))\n",
    "            except IndexError:\n",
    "                classification.setLeft(self.train(pd.DataFrame(), attributes,self.mode(yVector))) \n",
    "                \n",
    "            try:\n",
    "                examples_1 = examples.loc[examples[best_feature] == unique_values_best[1]]\n",
    "                examples_1 = examples_1.drop(best_feature, 1)\n",
    "                yVector1 = examples_1.iloc[: , -1:]\n",
    "                modeExamples1 = self.mode(yVector1)\n",
    "                classification.setRight(self.train(examples_1, attributes,modeExamples1))\n",
    "            except IndexError:\n",
    "                classification.setRight(self.train(pd.DataFrame(), attributes,self.mode(yVector))) \n",
    "            classification.setValue(best_feature)\n",
    "            return classification\n",
    "                \n",
    "            \n",
    "    #return mode of the dataframe with input y-vector of examples\n",
    "    def mode(self, yVector):\n",
    "        realMode = stats.mode(yVector).mode\n",
    "        return realMode\n",
    "    \n",
    "    #returns best attribute and index\n",
    "    def choose_bestAttribute(self, attributes, examples):\n",
    "        entropies = []\n",
    "        small_random_number = np.finfo(float).eps #this is added here to fix the divide by zero issues\n",
    "        for i in range(len(attributes)-1):\n",
    "            zeroRows = examples[(examples.iloc[:, i] == 0)]\n",
    "            num_rows_zeroRows, num_cols_zeroCols = zeroRows.shape\n",
    "            \n",
    "            oneRows = examples[(examples.iloc[:, i] == 1)]\n",
    "            num_rows_oneRows, num_cols_oneCols = oneRows.shape\n",
    "            \n",
    "            yZeroVecZero = zeroRows[(zeroRows.iloc[:, -1] == 0)]\n",
    "            num_rows_yZeroVecZero, num_cols_yZeroVecZero = yZeroVecZero.shape\n",
    "            \n",
    "            yZeroVecOne = zeroRows[(zeroRows.iloc[:, -1] == 1)]\n",
    "            num_rows_yZeroVecOne, num_cols_yZeroOne = yZeroVecOne.shape\n",
    "            \n",
    "            yOneVecZero = oneRows[(oneRows.iloc[:, -1] == 0)]\n",
    "            num_rows_yOneVecZero, num_cols_yOneVecZero = yOneVecZero.shape\n",
    "            \n",
    "            yOneVecOne = oneRows[(oneRows.iloc[:, -1] == 1)]\n",
    "            num_rows_yOneVecOne, num_cols_yOneVecOne = yOneVecOne.shape\n",
    "            \n",
    "            entropy1 = (num_rows_zeroRows/len(examples)) * (((-num_rows_yZeroVecZero/(num_rows_zeroRows+small_random_number)) * np.log(num_rows_yZeroVecZero/(num_rows_zeroRows+small_random_number) + small_random_number)) + ((-num_rows_yZeroVecOne/(num_rows_zeroRows+small_random_number)) * np.log(num_rows_yZeroVecOne/(num_rows_zeroRows+small_random_number) + small_random_number)))\n",
    "            entropy2 = (num_rows_oneRows/len(examples)) * (((-num_rows_yOneVecZero/(num_rows_oneRows+small_random_number)) * np.log(num_rows_yOneVecZero/(num_rows_oneRows+small_random_number) + small_random_number )) + ((-num_rows_yOneVecOne/(num_rows_oneRows+small_random_number))* np.log(num_rows_yOneVecOne/(num_rows_oneRows+small_random_number) + small_random_number)))\n",
    "                     \n",
    "            entropies.append(entropy1+entropy2)\n",
    "            \n",
    "        indexOfBestFeature = entropies.index(np.min(entropies))\n",
    "        return examples.columns[indexOfBestFeature], indexOfBestFeature\n",
    "\n",
    "\n",
    "    \n",
    "    #use tree predict testingData (examples in this case is known as the testing data continuous use)\n",
    "    def test(self, tree, testingDataWithHeaders):\n",
    "        yHat = []\n",
    "        for i in range(len(testingDataWithHeaders)):\n",
    "            currentHead = tree\n",
    "            x = True\n",
    "            while x:\n",
    "                #Go to direction the value of currentHead is\n",
    "                featureName = currentHead.getValue()\n",
    "                val = testingDataWithHeaders[featureName].iloc[i] #either 0 or 1\n",
    "                if val == 0: #this is the branch \n",
    "                    currentHead = currentHead.getLeft()\n",
    "                else:\n",
    "                    currentHead = currentHead.getRight()\n",
    "                    \n",
    "                if (currentHead.getValue() == 0) or (currentHead.getValue() == 1): #to check for base condition\n",
    "                    x = False\n",
    "                    yVal = 0 if (currentHead.getValue() == 0) else 1\n",
    "            yHat.append(yVal)\n",
    "        return yHat\n",
    "            \n",
    "        \n",
    "    def getFinalTree(self):\n",
    "        return self.tree\n",
    "    \n",
    "    def setFinalTree(self, tr):\n",
    "        self.tree = tr\n",
    "    \n",
    "\n",
    "def binarizeFeature(data):\n",
    "    mean = np.mean(data)\n",
    "#     mean = np.median(data)\n",
    "    ans = (data > mean) * 1\n",
    "    return ans\n",
    "\n",
    "\n",
    "#   header to dataframe\n",
    "def addHeader(data):\n",
    "    num_rows, num_cols = data.shape\n",
    "    feature_names = []\n",
    "    for i in range(num_cols):\n",
    "        if i == num_cols:\n",
    "            break\n",
    "        feature_names.append(\"feature\"+str(i))\n",
    "    data.columns = feature_names\n",
    "\n",
    "\n",
    "\n",
    "def decisionTree(filename):\n",
    "    # Read in data, ignore first row (header info) and first column (index info)\n",
    "    df = pd.read_csv(filename, header=None)\n",
    "\n",
    "    num_rows, num_cols = df.shape\n",
    "    \n",
    "    # Randomize the data (Gives the same data everytime)\n",
    "    df = df.sample(n = num_rows, random_state=0)\n",
    "    \n",
    "    # Select 2/3 of the data for training and the rest for testing\n",
    "    train = df.sample(frac=0.667, random_state=0)\n",
    "    test = df.drop(train.index)\n",
    "    \n",
    "    # Standadize the data... Make sure you retain the mean and standard deviation\n",
    "    mean = np.mean(df).iloc[:-1]\n",
    "    std = np.std(df, axis=0, ddof=1).iloc[:-1]\n",
    "    trainXVec = train.iloc[: , :-1] #X-vector train\n",
    "    testXVec = test.iloc[: , :-1] #X-vector test\n",
    "    trainYVec = train.iloc[: , -1:] #Y-vector train\n",
    "    testYVec = test.iloc[: , -1:] #Y-vector test\n",
    "    sTrainXVec = (trainXVec - mean)/std    \n",
    "    sTestXVec = (testXVec - mean)/std\n",
    "    \n",
    "    #combine standardized X-vector of train with untouched Y-vector\n",
    "    binaryTrain = binarizeFeature(sTrainXVec)\n",
    "    binaryTest = binarizeFeature(sTestXVec)\n",
    "    \n",
    "    finalTraining = binaryTrain.join(trainYVec)\n",
    "    finalTesting = binaryTest.join(testYVec)\n",
    "    \n",
    "    addHeader(finalTraining)\n",
    "    addHeader(finalTesting)\n",
    "    attributes = list(finalTraining.columns.values)\n",
    "    dt = DecisionTree()\n",
    "    finalTree = dt.train(finalTraining, attributes, None)\n",
    "    yPred = dt.test(finalTree,finalTesting)\n",
    "    \n",
    "    TP=0\n",
    "    FP=0\n",
    "    TN=0\n",
    "    FN=0\n",
    "    \n",
    "    ##Get the testing statistics\n",
    "    for i in range(len(finalTesting)):\n",
    "        predicted = yPred[i]\n",
    "        given = testYVec.iloc[i,0]\n",
    "        if predicted == 1:\n",
    "            if given == 1:\n",
    "                TP = TP + 1\n",
    "            else:\n",
    "                FP = FP + 1\n",
    "        else:\n",
    "            if given == 0:\n",
    "                TN = TN + 1\n",
    "            else:\n",
    "                FN = FN + 1 \n",
    "\n",
    "    Precision = TP/(TP + FP)\n",
    "    Recall = TP/(TP + FN)\n",
    "    F_measure = (2 * Precision * Recall)/(Precision + Recall)\n",
    "    Accuracy = (TP + TN)/(TP + TN + FP + FN)\n",
    "    \n",
    "    print(\"Precision = %.4f \" %(Precision))\n",
    "    print(\"Recall = %.4f \" %(Recall))\n",
    "    print(\"F_measure = %.4f \" %(F_measure))\n",
    "    print(\"Accuracy = %.4f \" %(Accuracy))\n",
    "    # ---------------------------------------------------------------------------------------------------------\n",
    "    print(\"--------------------\")\n",
    "    \n",
    "    print(\"Number of mislabeled points out of a total %d points : %d\" % (len(finalTesting), (np.ravel(testYVec) != yPred).sum())) \n",
    "\n",
    "\n",
    "    \n",
    "decisionTree(\"spambase.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   -0.117781\n",
      "1    0.207741\n",
      "2   -0.285016\n",
      "3   -0.114201\n",
      "4   -2.775650\n",
      "5   -0.779563\n",
      "6    0.549371\n",
      "7    1.383758\n",
      "8    0.711237\n",
      "9    1.220103\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X1 = np.array([-2,-5,-3,0,-8,-2,1,5,-1,6])\n",
    "X2 = np.array([1,-4,1,3,11,5,0,-1,-3,1])\n",
    "df = pd.DataFrame({'x1':X1, 'x2':X2})\n",
    "mean = np.mean(df)\n",
    "std = np.std(df, axis=0, ddof=1)\n",
    "# print()\n",
    "# print(std)\n",
    "\n",
    "sX = (df - mean)/std\n",
    "# cov_matrix = (sX.T @ sX)/(len(sX)-1)\n",
    "cov_matrix = np.cov(sX, rowvar=0)\n",
    "val, vec = np.linalg.eig(cov_matrix)\n",
    "idx = np.argsort(-1 * val)\n",
    "val = val[idx]\n",
    "vec = vec[:,idx]\n",
    "pc1_eigenVectors = vec[:,0]\n",
    "projection = sX @ pc1_eigenVectors\n",
    "print(projection)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
